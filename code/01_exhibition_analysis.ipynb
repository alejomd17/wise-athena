{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42c29f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebec33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# ============================================\n",
    "sales = pd.read_csv('../data/sales_transactions.csv').drop_duplicates()\n",
    "exhibitions = pd.read_csv('../data/store_exhibitions.csv').drop_duplicates()\n",
    "stores = pd.read_csv('../data/store_metadata.csv').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d55e4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "# ============================================\n",
    "# Transform datetime columns\n",
    "sales['Date'] = pd.to_datetime(sales['Date'])\n",
    "sales['Year'] = sales['Date'].dt.isocalendar().year\n",
    "sales['Week'] = sales['Date'].dt.isocalendar().week\n",
    "\n",
    "# I create revenue column\n",
    "sales['Revenue'] = sales['Price'] * sales['Units_Sold']\n",
    "\n",
    "# as Exhibition is in week I transform Sales ina week df too\n",
    "weekly_agg = sales.groupby(['Year', 'Week', 'Store_Id', 'Sku_Id']).agg({\n",
    "    'Revenue': 'sum',\n",
    "    'Units_Sold': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# The price is a Weighted Price, to give importance the units sold\n",
    "weekly_agg['Price'] = weekly_agg['Revenue'] / weekly_agg['Units_Sold']\n",
    "\n",
    "# Delete Revenue, I don't need it\n",
    "weekly_agg.drop(columns=['Revenue'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a97d96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Duplicated records =================\n",
      "        Year  Week  Store_Id  Sku_Id  Method_1  Method_2  Method_3\n",
      "10182   2023    14      3061     182       0.0       1.0       0.0\n",
      "10183   2023    14      3061     182       0.0       0.0       1.0\n",
      "70056   2023    25       111     728       0.0       1.0       0.0\n",
      "70057   2023    25       111     728       0.0       0.0       1.0\n",
      "76584   2023    26      1169     182       0.0       0.0       1.0\n",
      "76585   2023    26      1169     182       0.0       1.0       0.0\n",
      "427519  2024    34       113     727       0.0       1.0       1.0\n",
      "427520  2024    34       113     727       0.0       0.0       1.0\n",
      "457706  2024    39      1438     182       0.0       1.0       1.0\n",
      "457707  2024    39      1438     182       0.0       0.0       1.0\n",
      "559673  2025     4      2922     182       0.0       1.0       0.0\n",
      "559674  2025     4      2922     182       0.0       0.0       1.0\n",
      "\n",
      "Total Records without drop duplicates: 619706\n",
      "Total Records drop duplicates: 619694\n",
      "Records deleted: 12\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# ============================================\n",
    "\n",
    "# I merge weekly sales with exhibitions and stores\n",
    "df = pd.merge(weekly_agg, exhibitions, on=['Year', 'Week', 'Store_Id', 'Sku_Id'], how='left')\n",
    "df[['Method_1', 'Method_2', 'Method_3']] = df[['Method_1', 'Method_2', 'Method_3']].fillna(0)\n",
    "\n",
    "# Clean atipics\n",
    "# ============================================\n",
    "# In this case I note that exist Year-Week-Storage-Sku that has more than one method. \n",
    "\n",
    "duplicated_rows = df[df.duplicated(subset=['Year', 'Week', 'Store_Id', 'Sku_Id'], keep=False)]\n",
    "# Ver algunos ejemplos de duplicados\n",
    "print(f\"{'='*17} Duplicated records {'='*17}\")\n",
    "print(duplicated_rows.sort_values(['Year', 'Week', 'Store_Id', 'Sku_Id'])[['Year', 'Week', 'Store_Id', 'Sku_Id', 'Method_1', 'Method_2', 'Method_3']])\n",
    "# I dropped it, because it is not common, \n",
    "# But, it can be an analysis by itself creating a \"Combo\" method.\n",
    "size_df = len(df)\n",
    "print(f\"\\nTotal Records without drop duplicates: {size_df}\")\n",
    "\n",
    "df = df.drop(index=duplicated_rows.index)\n",
    "size_df_clean = len(df)\n",
    "\n",
    "print(f\"Total Records drop duplicates: {len(df)}\")\n",
    "print(f\"Records deleted: {size_df - size_df_clean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c755e78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DISTRIBUCIÓN DE MÉTODOS DE EXHIBICIÓN ===\n",
      "Method_1: 0.04% de cobertura\n",
      "Method_2: 2.14% de cobertura\n",
      "Method_3: 30.47% de cobertura\n",
      "\n",
      "=== VENTAS PROMEDIO CON/SIN MÉTODOS ===\n",
      "Method_1: 152.49 vs 118.57 → +28.61%\n",
      "Method_2: 152.53 vs 117.84 → +29.44%\n",
      "Method_3: 163.35 vs 98.97 → +65.06%\n"
     ]
    }
   ],
   "source": [
    "# Análisis básico de métodos de exhibición\n",
    "print(\"=== DISTRIBUCIÓN DE MÉTODOS DE EXHIBICIÓN ===\")\n",
    "for method in ['Method_1', 'Method_2', 'Method_3']:\n",
    "    coverage = df[method].mean() * 100\n",
    "    print(f\"{method}: {coverage:.2f}% de cobertura\")\n",
    "\n",
    "# Relación métodos vs ventas\n",
    "print(\"\\n=== VENTAS PROMEDIO CON/SIN MÉTODOS ===\")\n",
    "for method in ['Method_1', 'Method_2', 'Method_3']:\n",
    "    with_method = df[df[method] == 1]['Units_Sold'].mean()\n",
    "    without_method = df[df[method] == 0]['Units_Sold'].mean()\n",
    "    uplift = (with_method / without_method - 1) * 100\n",
    "    print(f\"{method}: {with_method:.2f} vs {without_method:.2f} → {uplift:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da618a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             Units_Sold   R-squared:                       0.088\n",
      "Model:                            OLS   Adj. R-squared:                  0.088\n",
      "Method:                 Least Squares   F-statistic:                 1.502e+04\n",
      "Date:                Tue, 02 Sep 2025   Prob (F-statistic):               0.00\n",
      "Time:                        23:38:32   Log-Likelihood:            -3.7151e+06\n",
      "No. Observations:              619694   AIC:                         7.430e+06\n",
      "Df Residuals:                  619689   BIC:                         7.430e+06\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        103.4470      0.214    483.218      0.000     103.027     103.867\n",
      "Price         -0.0856      0.003    -33.008      0.000      -0.091      -0.080\n",
      "Method_1      28.3934      6.260      4.536      0.000      16.124      40.663\n",
      "Method_2      30.2200      0.854     35.398      0.000      28.547      31.893\n",
      "Method_3      68.4145      0.297    230.245      0.000      67.832      68.997\n",
      "==============================================================================\n",
      "Omnibus:                   963047.847   Durbin-Watson:                   1.465\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):      16899846889.910\n",
      "Skew:                           8.624   Prob(JB):                         0.00\n",
      "Kurtosis:                     811.834   Cond. No.                     4.63e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.63e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "=== IMPACTO DE EXHIBICIONES EN VENTAS ===\n",
      "Method_1: +28.4 (p-value: 0.0000) ✓ SIGNIFICATIVO\n",
      "Method_2: +30.2 (p-value: 0.0000) ✓ SIGNIFICATIVO\n",
      "Method_3: +68.4 (p-value: 0.0000) ✓ SIGNIFICATIVO\n"
     ]
    }
   ],
   "source": [
    "# Preparar variables para el modelo\n",
    "features = ['Price', 'Method_1', 'Method_2', 'Method_3']\n",
    "X = df[features]\n",
    "X_ols = sm.add_constant(X)  # Intercept\n",
    "y = df['Units_Sold']\n",
    "\n",
    "# MOdelo lineal de regresión\n",
    "model = sm.OLS(y, X_ols).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# Interpretar resultados en términos business-friendly\n",
    "print(\"\\n=== IMPACTO DE EXHIBICIONES EN VENTAS ===\")\n",
    "for method in ['Method_1', 'Method_2', 'Method_3']:\n",
    "    if method in model.params:\n",
    "        coef = model.params[method]\n",
    "        pval = model.pvalues[method]\n",
    "\n",
    "        if pval < 0.05:\n",
    "            significance = \"✓ SIGNIFICATIVO\"\n",
    "        else:\n",
    "            significance = \"✗ NO SIGNIFICATIVO\"\n",
    "        \n",
    "        print(f\"{method}: {coef:+.1f} (p-value: {pval:.4f}) {significance}\")\n",
    "\n",
    "mse_ols = mean_squared_error(y,  model.predict(X_ols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fc46b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             Units_Sold   R-squared:                       0.101\n",
      "Model:                            OLS   Adj. R-squared:                  0.101\n",
      "Method:                 Least Squares   F-statistic:                 1.744e+04\n",
      "Date:                Tue, 02 Sep 2025   Prob (F-statistic):               0.00\n",
      "Time:                        23:38:14   Log-Likelihood:            -6.8256e+05\n",
      "No. Observations:              619694   AIC:                         1.365e+06\n",
      "Df Residuals:                  619689   BIC:                         1.365e+06\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          4.3700      0.002   2723.568      0.000       4.367       4.373\n",
      "Price         -0.0004   1.94e-05    -21.649      0.000      -0.000      -0.000\n",
      "Method_1       0.2113      0.047      4.504      0.000       0.119       0.303\n",
      "Method_2       0.2307      0.006     36.057      0.000       0.218       0.243\n",
      "Method_3       0.5430      0.002    243.805      0.000       0.539       0.547\n",
      "==============================================================================\n",
      "Omnibus:                    49013.072   Durbin-Watson:                   1.330\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           104870.635\n",
      "Skew:                          -0.521   Prob(JB):                         0.00\n",
      "Kurtosis:                       4.725   Cond. No.                     4.63e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.63e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "=== IMPACTO DE EXHIBICIONES EN VENTAS ===\n",
      "Method_1: +23.53% (p-value: 0.0000) ✓ SIGNIFICATIVO\n",
      "Method_2: +25.95% (p-value: 0.0000) ✓ SIGNIFICATIVO\n",
      "Method_3: +72.11% (p-value: 0.0000) ✓ SIGNIFICATIVO\n"
     ]
    }
   ],
   "source": [
    "# Modelo logaritmico de regresión\n",
    "y_log = np.log(df['Units_Sold'] + 1)  # Transformación logarítmica\n",
    "model_log = sm.OLS(y_log, X_ols).fit()\n",
    "print(model_log.summary())\n",
    "\n",
    "# Interpretar resultados en términos business-friendly\n",
    "print(\"\\n=== IMPACTO DE EXHIBICIONES EN VENTAS ===\")\n",
    "for method in ['Method_1', 'Method_2', 'Method_3']:\n",
    "    if method in model_log.params:\n",
    "        coef = model_log.params[method]\n",
    "        pval = model_log.pvalues[method]\n",
    "        effect_pct = (np.exp(coef) - 1) * 100\n",
    "        \n",
    "        if pval < 0.05:\n",
    "            significance = \"✓ SIGNIFICATIVO\"\n",
    "        else:\n",
    "            significance = \"✗ NO SIGNIFICATIVO\"\n",
    "        \n",
    "        print(f\"{method}: {effect_pct:+.2f}% (p-value: {pval:.4f}) {significance}\")\n",
    "\n",
    "mse_log = mean_squared_error(y,  model_log.predict(X_ols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f523b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree MSE: 8214.94\n"
     ]
    }
   ],
   "source": [
    "# impacto de las exhibiciones sobre las ventas\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_model = DecisionTreeRegressor(random_state=42, max_depth=4)\n",
    "tree_model.fit(X, y)\n",
    "\n",
    "# Predicción y error\n",
    "y_pred_tree = tree_model.predict(X)\n",
    "mse_tree = mean_squared_error(y, y_pred_tree)\n",
    "print(f'Decision Tree MSE: {mse_tree:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "680906d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MSE: 2388.45\n",
      "\n",
      "Random Forest Feature Importances:\n",
      "Price       0.896049\n",
      "Method_3    0.095462\n",
      "Method_2    0.008283\n",
      "Method_1    0.000207\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Predicción y error\n",
    "y_pred_rf = rf_model.predict(X)\n",
    "mse_rf = mean_squared_error(y, y_pred_rf)\n",
    "print(f'Random Forest MSE: {mse_rf:.2f}')\n",
    "\n",
    "# Importancia de variables\n",
    "importances_rf = pd.Series(rf_model.feature_importances_, index=features)\n",
    "print(\"\\nRandom Forest Feature Importances:\")\n",
    "print(importances_rf.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f3dc320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MSE: 7791.41\n",
      "\n",
      "XGBoost Feature Importances:\n",
      "Method_3    0.767367\n",
      "Price       0.214457\n",
      "Method_2    0.016872\n",
      "Method_1    0.001305\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X)\n",
    "mse_xgb = mean_squared_error(y, y_pred_xgb)\n",
    "print(f'XGBoost MSE: {mse_xgb:.2f}')\n",
    "\n",
    "# Importancia\n",
    "importances_xgb = pd.Series(xgb_model.feature_importances_, index=features)\n",
    "print(\"\\nXGBoost Feature Importances:\")\n",
    "print(importances_xgb.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b9e1780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 261\n",
      "[LightGBM] [Info] Number of data points in the train set: 619694, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 118.584998\n",
      "LightGBM MSE: 7771.86\n",
      "\n",
      "LightGBM Feature Importances:\n",
      "Price       2473\n",
      "Method_3     283\n",
      "Method_2     224\n",
      "Method_1      20\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "lgb_model.fit(X, y)\n",
    "\n",
    "y_pred_lgb = lgb_model.predict(X)\n",
    "mse_lgb = mean_squared_error(y, y_pred_lgb)\n",
    "print(f'LightGBM MSE: {mse_lgb:.2f}')\n",
    "\n",
    "# Importancia\n",
    "importances_lgb = pd.Series(lgb_model.feature_importances_, index=features)\n",
    "print(\"\\nLightGBM Feature Importances:\")\n",
    "print(importances_lgb.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8805068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparación de MSE:\n",
      "- Árbol de Decisión: 8214.94\n",
      "- Random Forest: 2388.45\n",
      "- XGBoost: 7791.41\n",
      "- LightGBM: 7771.86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Comparación de MSE:\n",
    "- Árbol de Decisión: {mse_tree:.2f}\n",
    "- Random Forest: {mse_rf:.2f}\n",
    "- XGBoost: {mse_xgb:.2f}\n",
    "- LightGBM: {mse_lgb:.2f}\n",
    "\"\"\")\n",
    "\n",
    "dict_mse = {'DT':mse_tree,\n",
    "'RF':mse_rf,\n",
    "'XGB':mse_xgb,\n",
    "'LGB':mse_lgb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c8bbeec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo es: RF con MSE = 2388.45\n",
      "\n",
      "Importancia de variables en el mejor modelo:\n",
      "Price       0.896049\n",
      "Method_3    0.095462\n",
      "Method_2    0.008283\n",
      "Method_1    0.000207\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RF'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mse_dict = {'DT':mse_tree,\n",
    "'RF':mse_rf,\n",
    "'XGB':mse_xgb,\n",
    "'LGB':mse_lgb}\n",
    "\n",
    "model_dict = {\n",
    "    'DT': tree_model,\n",
    "    'RF': rf_model,\n",
    "    'XGB': xgb_model,\n",
    "    'LGB': lgb_model\n",
    "}\n",
    "\n",
    "best_model_name = min(dict_mse, key=dict_mse.get)\n",
    "print(f\"El mejor modelo es: {best_model_name} con MSE = {dict_mse[best_model_name]:.2f}\")\n",
    "\n",
    "best_model = model_dict[best_model_name]\n",
    "\n",
    "# Paso 3: obtener la importancia de variables\n",
    "importances = pd.Series(best_model.feature_importances_, index=features)\n",
    "importances = importances.sort_values(ascending=False)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nImportancia de variables en el mejor modelo:\")\n",
    "print(importances)\n",
    "\n",
    "best_model_name = min(dict_mse, key=dict_mse.get)\n",
    "best_model_name\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_wise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
